
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title>R12/Ishank Juneja/16D070012</title>
  <style type="text/css">code{white-space: pre;}</style>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
</head>
<body>
<div id="header">
<h1 class="title">R12/Ishank Juneja/16D070012</h1>
</div>
<h2 id="introduction">Introduction</h2>
<p>Under the classic Markov Decision Process (MDP) setup, we associate a tuple <span class="math inline">\((S, A, T, R)\)</span> with every MDP. In this notation, <span class="math inline">\(S\)</span> is the set of states the agent can take, <span class="math inline">\(A\)</span> is the set of possible actions for the agent, <span class="math inline">\(T:S 	imes A ightarrow \Pi(s)\)</span> is the state transition function of the problem. <span class="math inline">\(T\)</span> specifies the probability distribution over next states <span class="math inline">\(s&#39; \in S\)</span> on taking an action <span class="math inline">\(a \in A\)</span> starting from state <span class="math inline">\(s \in S\)</span>. <span class="math inline">\(R\)</span> is the reward function which can be viewed as a feedback on agent performance from the environment. An assumption implicit to this model is the observability of state <span class="math inline">\(s \in S\)</span>. Often times, it is not the case that an agent has access to its exact state. Instead, the agent might only have access to an observation <span class="math inline">\(o \in \Omega\)</span> which captures incomplete information about its underlying state. This framework satisfactorily models many practical situations. For instance, consider a robot that is attempting to localize its position (directly unobservable state <span class="math inline">\(s\)</span>) in a multi-storied building by merely looking at a particular intersection in a corridor (observation <span class="math inline">\(o\)</span>). Partially Observable MDPs (POMDPs) provide a richer mathematical framework which is able to describe many such situations.</p>
<h2 id="the-pomdp-framework" class="unnumbered">The POMDP Framework</h2>
<p>A POMDP is completely described by the tuple <span class="math inline">\((S, A, T, R, \Omega, O)\)</span>, where<br />
<span class="math inline">\((S, A, T, R)\)</span> is the underlying MDP, <span class="math inline">\(\Omega\)</span> is a finite set of observations the agent can experience and <span class="math inline">\(O:S 	imes A ightarrow \Pi(\Omega)\)</span> is the observation function which gives, for every state and action, a probability distribution over possible observations. Under the POMDP framework an agent is unable to discern its state accurately. Instead of the true state, the agent makes “an observation” <span class="math inline">\(o\)</span> based on the action <span class="math inline">\(a\)</span> taken and the (still unobserved) resulting state <span class="math inline">\(s&#39;\)</span>.<br />
An implication of this limitation is that an agent wishing to act optimally must use its memory of all previous actions and observations. To simplify the problem, the POMDP model introduces a <em>belief state</em> <span class="math inline">\(b\)</span>. Belief states are probability distributions over the true underlying states of the agent. That is, for each state <span class="math inline">\(s\)</span>, <span class="math inline">\(b(s)\)</span> gives the probability of the agent being in that state. By design, the agents belief state at any point of time is a sufficient statistic (SS) for all its previous experience. As a SS, the belief states capture all the past observations and the belief state that the agent started with. Sequential decision-making in the POMDP becomes Markovian in belief state <span class="math inline">\(b\)</span> due to the SS property.<br />
To compute and update belief states as new experience is accumulated, the agent includes a block known as the state-estimator (SE). More precisely, given an observation <span class="math inline">\(o\)</span>, the action taken <span class="math inline">\(a\)</span> and the previous belief state <span class="math inline">\(b\)</span>, the SE is the function SE<span class="math inline">\((b, a, o)\)</span> which outputs the new belief state <span class="math inline">\(b&#39;\)</span>. Using the rules of conditional probability and the definitions of the POMDP parameters, the paper derives the relation <span class="math display">\[b&#39;(s&#39;) \propto O(s&#39;, a, o)\sum_{s \in S}T(s, a, s&#39;)b(s),\]</span> where the probabilities in <span class="math inline">\(b&#39;\)</span> are computed using the constraint <span class="math inline">\(\sum_{s \in S}b(s) = 1\)</span>.<br />
The other necessary component to describe a POMDP agent is the policy being followed by it. In the POMDP framework, a policy maps a belief state vector <span class="math inline">\(b\)</span> to an action <span class="math inline">\(a\)</span>. Since the problem is Markovian in the belief state <span class="math inline">\(b\)</span>, obtaining a policy through POMDP planning is equivalent to solving the planning problem for an associated <strong>belief MDP</strong>.<br />
The parameters for the belief MDP are described below,</p>
<ul>
<li><p><span class="math inline">\(\mathcal{B}\)</span> is the set of possible belief states and forms the state space</p></li>
<li><p>The set of actions <span class="math inline">\(A\)</span> remains the same as the POMDP</p></li>
<li><p><span class="math inline">\(	au(b,a,b&#39;)\)</span> is the state-transition function which can be computed using the POMDP parameters</p></li>
<li><p><span class="math inline">\(ho(b, a)\)</span> is the reward function and is given by an expectation over the reward function <span class="math inline">\(R(s, a)\)</span> under the distribution over states given by <span class="math inline">\(b(s)\)</span></p></li>
</ul>
<h2 id="planning-for-pomdps" class="unnumbered">Planning for POMDPs</h2>
<p>It is often intractable to solve the planning problem for continuous state MDPs, however the special structure of the belief MDP lends it properties that give rise to efficient planning algorithms. In this paper the authors focus primarily on two planning approaches - <strong>Exhaustive Enumeration and Witness Algorithm</strong> the latter among which is a novel approach. Both methods provide a means to determine the optimal <span class="math inline">\(t\)</span> - step value function <span class="math inline">\(V_t\)</span> given a start belief state <span class="math inline">\(b\)</span>. Here <span class="math inline">\(t\)</span> - step refers to the situation where there are only <span class="math inline">\(t\)</span> steps remaining in the agents lifetime. The paper considers the more general finite horizon problem since an infinite horizon discounted trajectory can always be approximated to arbitrary precision by a long enough finite trajectory. Lastly, once the optimal value function is computed using Value-Iteration, it is a straightforward task to determine the optimal policy to complete planning.<br />
Next, the paper describes how a <span class="math inline">\(t\)</span>-step policy can be represented using a <em>policy tree</em> - <span class="math inline">\(p\)</span> of depth <span class="math inline">\(t\)</span>. An expression for the expected return <span class="math inline">\(V_p(s)\)</span> associated with executing the policy <span class="math inline">\(p\)</span>, starting from a state <span class="math inline">\(s\)</span>, can be derived using the belief MDP parameters. However, the actual quantity of interest is the return for a given <em>belief state</em> <span class="math inline">\(b\)</span>. Using the linearity of expectations we have, <span class="math inline">\(V_p(b) = \sum_{s \in S} b(s) V_p(s)\)</span>. Treating <span class="math inline">\(b\)</span> and <span class="math inline">\(V_p\)</span> as vectors of length <span class="math inline">\(|S|\)</span>, <span class="math inline">\(V_p\)</span> becomes a dot product.<br />
The objective of planning through value-iteration is to obtain <span class="math display">\[V_t(b) = \max_{p \in \mathcal{P}} b 